#1
(a)
RDD=sc.textFile("/labdata/sheakspear.txt")
RDD.take(10)

(b)
RDD1=RDD.flatMap(lambda x: x.split(" "))
RDD1.take(10)

(c) 
RDD2=RDD.zipWithIndex().map(lambda x: (x[1] ,x[0]))
RDD2.take(10)

(d) 
RDD3=RDD.zipWithIndex().map(lambda x: (x[1] ,x[0].split(" ")))
RDD3.take(10)

(e)
rdd49=RDD.zipWithIndex().flatMap(lambda x: ((x[1],(i,1)) for i in x[0].split(" ")))
rdd4=rdd49.map(lambda x: ((x[0], x[1][0]), x[1][1])).reduceByKey(lambda x,y: x+y)

RDD4=rdd4.map(lambda x: (x[0][0], (x[0][1], x[1]))).reduceByKey(lambda x, y: x+y)
RDD4.take(10)

(f) 
RDD5=rdd4.map(lambda x: (x[0][1], (x[0][0], x[1]))).reduceByKey(lambda x,y:(x+y))
RDD5.take(10)

(g) 
RDD6=rdd4.map(lambda x: (x[0][1], [(x[0][0], x[1])])).reduceByKey(lambda x,y: x+y)
RDD6.take(10)

(h) 
rdd7=RDD.zipWithIndex().flatMap(lambda x: ((i,x[1]) for i in x[0].split(" ")))
rdd7=rdd7.map(lambda x: (x[0], [x[1]]))
RDD7=rdd7.reduceByKey(lambda x,y: x+y)
RDD7.take(10)

(i)
RDD8=RDD1.map(lambda x:(x,1)).reduceByKey(lambda x,y:(x+y))
RDD8.take(10)

(j)
RDD9=rdd49.map(lambda x: ((x[1][0], x[0]), x[1][1])).reduceByKey(lambda x,y: x+y)
RDD9.take(10)

(k)
RDD10=RDD9.count()
RDD10

(l)
RDD11=rdd49.map(lambda x: (x[1][0], (x[0], x[1][1]))).reduceByKey(lambda x,y: x+y)
RDD11.take(10)

(m)
RDD12=RDD7.join(RDD8)
RDD12.take(10)


#2
(a)
RDDa=sc.textFile("/labdata/nyctaxisub.txt")
RDDa.take(10)

(b)
df = sqlContext.read.load("/labdata/nyctaxisub.txt", format='com.databricks.spark.csv', header='true', inferSchema='true')
df.dtypes
df.show()

(c)
from pyspark.sql import functions as F
c=df.filter(F.col("dropoff_datetime").between('2013-02-09 00:00:00','2013-02-11 23:59:59'))
C=c.orderBy(F.col("passenger_count").desc()).select("medallion").show()

(d)
D=c.orderBy(F.col("passenger_count").desc()).select("medallion").count()
D

(e)
e=df.filter(F.col("trip_time_in_secs")>(F.lit("900")))
E=e.orderBy(F.col("passenger_count").desc()).select("medallion").show()
